{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PnNaDb2ZTCjD"
   },
   "outputs": [],
   "source": [
    "order_products_train = pd.read_csv(\"order_products__train.csv\")\n",
    "order_products_prior = pd.read_csv(\"order_products__prior.csv\")\n",
    "orders = pd.read_csv(\"orders.csv\")\n",
    "products = pd.read_csv(\"products.csv\")\n",
    "aisles = pd.read_csv(\"aisles.csv\")\n",
    "departments = pd.read_csv(\"departments.csv\")\n",
    "submissions = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqSVFaN3TN83"
   },
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o0Ci_6OTTJHC"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "we have seen from the graph that how user purchase pattern change  within a day as well as period of month. \n",
    "So, we thought to use this as feature .\n",
    "\n",
    "'''\n",
    "orders['reorder_pattern'] = 'daily'\n",
    "orders['reorder_pattern'][orders['days_since_prior_order'] <= 1] = 'daily'\n",
    "orders['reorder_pattern'][(orders['days_since_prior_order'] > 1) & (orders['days_since_prior_order'] <= 7)]  = 'weekly'\n",
    "orders['reorder_pattern'][(orders['days_since_prior_order'] > 7) & (orders['days_since_prior_order'] <= 15)]  = 'biweekly'\n",
    "orders['reorder_pattern'][(orders['days_since_prior_order'] > 15) & (orders['days_since_prior_order'] <= 30)]  = 'monthly'\n",
    "\n",
    "# Set a default value\n",
    "orders['name_of_day_time'] = 'Morning'\n",
    "orders['name_of_day_time'][(orders['order_hour_of_day'] > 12) & (orders['order_hour_of_day'] <= 17)]  = 'Afternoon'\n",
    "orders['name_of_day_time'][(orders['order_hour_of_day'] > 17) & (orders['order_hour_of_day'] <= 19)]  = 'evening'\n",
    "orders['name_of_day_time'][(orders['order_hour_of_day'] > 19) & (orders['order_hour_of_day'] <= 24)]  = 'night'\n",
    "orders['name_of_day_time'][(orders['order_hour_of_day'] > 0) & (orders['order_hour_of_day'] <= 4)]  = 'night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQTSvIQFcv2B"
   },
   "outputs": [],
   "source": [
    "# read the prior order file #\n",
    "prior_df = pd.read_csv(\"order_products__prior.csv\")\n",
    "prior_df1=prior_df[(prior_df['reordered'] == 1)]\n",
    "\n",
    "# merge with the orders file to get the user_id #\n",
    "prior_df1 = pd.merge(prior_df1, orders, how=\"inner\", on=\"order_id\")\n",
    "prior_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "6KHC3RwHZSX8",
    "outputId": "9b7bb709-89c0-4f0f-f1f0-759233ce4ac6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>reorder_pattern</th>\n",
       "      <th>name_of_day_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>33120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202279</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28985</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>202279</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>45918</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>202279</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>17794</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>202279</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>40141</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>202279</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  product_id  ...  reorder_pattern  name_of_day_time\n",
       "0         2       33120  ...         biweekly           Morning\n",
       "1         2       28985  ...         biweekly           Morning\n",
       "2         2       45918  ...         biweekly           Morning\n",
       "3         2       17794  ...         biweekly           Morning\n",
       "4         2       40141  ...         biweekly           Morning\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " reading the prior data of users ordering products where reordering status is 1 beacuse we are all concern about reorder purchasing items.\n",
    " \n",
    "''' \n",
    "prior_ = pd.read_csv(\"order_products__prior.csv\")\n",
    "prior_data=prior_[prior_.reordered == 1]\n",
    "\n",
    "# merge  the orders data with prior_data on order_id\n",
    "prior_data = pd.merge(prior_data, orders, how=\"inner\", on=\"order_id\")\n",
    "prior_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "REnrA7iIdhmB"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "since i am planning to use last 5 purchase item history as feature to boost the model so i am avoiding the train and test evals set.\n",
    "\n",
    "'''\n",
    "train_orders=orders[orders.eval_set != 'test']\n",
    "\n",
    "train_orders=train_orders[train_orders.eval_set != 'train']\n",
    "\n",
    "prior_past_data = pd.merge(order_products_prior, train_orders, how=\"inner\", on=\"order_id\")\n",
    "\n",
    "\n",
    "reorder_rank = prior_past_data[['user_id','order_number']]\n",
    "reorder_rank = reorder_rank.sort_values(by ='user_id')\n",
    "reorder_rank = reorder_rank.drop_duplicates()\n",
    "reorder_rank['rank'] = reorder_rank.groupby(['user_id'])['order_number'].rank(ascending=False,method='dense')\n",
    "\n",
    "#creating function to extract user prior history\n",
    "def recent_ordering_status(rank):\n",
    "  reorder_history = reorder_rank[(reorder_rank['rank'] == 1)]\n",
    "  data = pd.merge(prior_past_data, reorder_history, how=\"inner\", on=[\"user_id\", \"order_number\"])\n",
    "  data = data[['user_id','product_id','add_to_cart_order','days_since_prior_order','order_hour_of_day','reordered']]\n",
    "  data['time_in_sec'] = data['days_since_prior_order']*24*60*60 + data['order_hour_of_day']*60*60\n",
    "  data = data[[\"user_id\", \"product_id\",\"add_to_cart_order\" ,\"time_in_sec\",\"reordered\"]]\n",
    "  data = data.sort_values(by ='user_id')\n",
    "  data.columns = [\"user_id\", \"product_id\",\"add_to_cart_order\"+str(rank) ,\"time_in_sec\"+str(rank),\"reordered_latest\"+str(rank)]\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2TUva5iumvY"
   },
   "outputs": [],
   "source": [
    "# storing the last 5 ordering history\n",
    "data1 = recent_ordering_status(1)\n",
    "data2 = recent_ordering_status(2)\n",
    "data3 = recent_ordering_status(3)\n",
    "data4 = recent_ordering_status(4)\n",
    "data5 = recent_ordering_status(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3QRK1rDUljEU"
   },
   "outputs": [],
   "source": [
    "'''  \n",
    "since our problem statement is to predict which of the previously purchase item will be in user next order. so we need to know\n",
    "first which are the products that user had purchased in past and then we merge it with train data with class label 1 as reorder item and 0 \n",
    "as not reorder item.\n",
    "\n",
    "'''\n",
    "\n",
    "use_prod = prior_data[[\"order_id\", \"product_id\"]]\n",
    "ors  =  orders[['order_id','user_id']]\n",
    "use_prod1 = pd.merge(use_prod, ors, how=\"inner\", on='order_id')\n",
    "use_prod1 = use_prod1[['user_id','product_id']]\n",
    "use_prod1 = use_prod1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_FDNgx5m3t1"
   },
   "outputs": [],
   "source": [
    "# we are finding the number of times the particular product has been reorder by the user in past.\n",
    "reorder_count = prior_data.groupby([\"user_id\",\"product_id\"])[\"reordered\"].aggregate([\"count\"]).reset_index()\n",
    "reorder_count.columns = [\"user_id\", \"product_id\", \"reordered_count\"]\n",
    "\n",
    "# we are finding the number of times the particular product has been reorder in past.\n",
    "reorder_count_by_product = prior_data.groupby(\"product_id\")[\"reordered\"].aggregate([\"count\"]).reset_index()\n",
    "reorder_count_by_product.columns = [\"product_id\", \"product_reordered_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "KxIhWHewoIQG",
    "outputId": "71ac0dd8-631c-4002-8726-39ca0e553af6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>reorder_pattern</th>\n",
       "      <th>name_of_day_time</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "      <td>14947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "      <td>5707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "      <td>30881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "      <td>43633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>Morning</td>\n",
       "      <td>49302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  user_id eval_set  ...  reorder_pattern  name_of_day_time  product_id\n",
       "0         1   112108    train  ...         biweekly           Morning       14947\n",
       "1         1   112108    train  ...         biweekly           Morning        5707\n",
       "2         1   112108    train  ...         biweekly           Morning       30881\n",
       "3         1   112108    train  ...         biweekly           Morning       43633\n",
       "4         1   112108    train  ...         biweekly           Morning       49302\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order id  on which we have to perform training\n",
    "train_data = pd.read_csv(\"order_products__train.csv\", usecols=[\"order_id\"]).drop_duplicates()\n",
    "# merging trainig order id with order data\n",
    "train_data = pd.merge(train_data, orders, how=\"inner\", on=\"order_id\")\n",
    "train_data = pd.merge(train_data, use_prod1, how=\"inner\", on=\"user_id\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-O6VdfMZotEw",
    "outputId": "9d79e601-9edb-41b1-b165-7b7900a19b21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>reorder_pattern</th>\n",
       "      <th>name_of_day_time</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>36855</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>daily</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>21709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>36855</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>daily</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>47766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>36855</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>daily</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>13107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>36855</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>daily</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>21463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>36855</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>daily</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>38777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  user_id eval_set  ...  reorder_pattern  name_of_day_time  product_id\n",
       "0        17    36855     test  ...            daily         Afternoon       21709\n",
       "1        17    36855     test  ...            daily         Afternoon       47766\n",
       "2        17    36855     test  ...            daily         Afternoon       13107\n",
       "3        17    36855     test  ...            daily         Afternoon       21463\n",
       "4        17    36855     test  ...            daily         Afternoon       38777\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order id  on which we have to predict product which could be in user next order\n",
    "test_data = pd.read_csv(\"sample_submission.csv\", usecols=[\"order_id\"])\n",
    "# merging trainig order id with order data\n",
    "test_data = pd.merge(test_data, orders, how=\"inner\", on=\"order_id\")\n",
    "test_data = pd.merge(test_data, use_prod1, how=\"inner\", on=\"user_id\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kaDV54M2t-5x",
    "outputId": "716dad33-960e-4376-b8da-b63d66d67821"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  aisle_id  department_id\n",
       "0           1        61             19\n",
       "1           2       104             13\n",
       "2           3        94              7\n",
       "3           4        38              1\n",
       "4           5         5             13"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_ = pd.read_csv(\"products.csv\", usecols=[\"product_id\", \"aisle_id\", \"department_id\"])\n",
    "products_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NJnQiYIuECD"
   },
   "outputs": [],
   "source": [
    "# addiing aisles and depertment id to train and test data\n",
    "train_data = pd.merge(train_data, products_, how=\"inner\", on=\"product_id\")\n",
    "test_data = pd.merge(test_data, products_, how=\"inner\", on=\"product_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPxQYJa-uga6"
   },
   "outputs": [],
   "source": [
    "#merging the last 5 orderig history to train and test data to boost the model\n",
    "train_data = pd.merge(train_data, data5, how=\"left\", on=['user_id',\"product_id\"])\n",
    "train_data = pd.merge(train_data, data4, how=\"left\", on=['user_id',\"product_id\"])\n",
    "train_data = pd.merge(train_data, data3, how=\"left\", on=['user_id',\"product_id\"])\n",
    "train_data = pd.merge(train_data, data2, how=\"left\", on=['user_id',\"product_id\"])\n",
    "train_data = pd.merge(train_data, data1, how=\"left\", on=['user_id',\"product_id\"])\n",
    "\n",
    "test_data = pd.merge(test_data, data5, how=\"left\", on=['user_id',\"product_id\"])\n",
    "test_data = pd.merge(test_data, data4, how=\"left\", on=['user_id',\"product_id\"])\n",
    "test_data = pd.merge(test_data, data3, how=\"left\", on=['user_id',\"product_id\"])\n",
    "test_data = pd.merge(test_data, data2, how=\"left\", on=['user_id',\"product_id\"])\n",
    "test_data = pd.merge(test_data, data1, how=\"left\", on=['user_id',\"product_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuLmlcyeyJWD"
   },
   "outputs": [],
   "source": [
    "# addiing  depertment detail to train and test data\n",
    "train_data = pd.merge(train_data, departments, how=\"left\", on=\"department_id\")\n",
    "test_data = pd.merge(test_data, departments, how=\"left\", on=\"department_id\")\n",
    "\n",
    "# addiing  aisles detail to train and test data\n",
    "train_data = pd.merge(train_data, aisles, how=\"left\", on=\"aisle_id\")\n",
    "test_data = pd.merge(test_data, aisles, how=\"left\", on=\"aisle_id\")\n",
    "\n",
    "# addiing  reorder_count detail to train and test data\n",
    "train_data = pd.merge(train_data, reorder_count, how=\"left\", on=['user_id',\"product_id\"])\n",
    "test_data = pd.merge(test_data, reorder_count, how=\"left\", on=['user_id',\"product_id\"])\n",
    "\n",
    "# addiing  reorder_count_by_product detail to train and test data\n",
    "train_data = pd.merge(train_data, reorder_count_by_product, how=\"left\", on=\"product_id\")\n",
    "test_data = pd.merge(test_data, reorder_count_by_product, how=\"left\", on=\"product_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2kiqpjuzccw"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "as we have seen from the graph that there are some product which has be order so many times by user as well as there\n",
    "are some department and aisle name as well from which product has been be taken from frequently. so, i thougth to consider \n",
    "this as feature in my model built.\n",
    "\n",
    "'''\n",
    "\n",
    "product_count = pd.merge(prior_data, products, how=\"inner\", on=\"product_id\")\n",
    "k = product_count['product_name'].value_counts()\n",
    "\n",
    "k = dict(k) \n",
    "product_count_dict = {i:j/len(product_count) for i , j  in k.items()}\n",
    "\n",
    "####################\n",
    "\n",
    "aisles_count = pd.merge(prior_data, products, how=\"inner\", on=\"product_id\")\n",
    "aisles_count = pd.merge(aisles_count, aisles, how=\"inner\", on=\"aisle_id\")\n",
    "k = aisles_count['aisle'].value_counts()\n",
    "\n",
    "k = dict(k) \n",
    "aisles_count_dict = {i:j/len(aisles_count) for i , j  in k.items()}\n",
    "\n",
    "###################\n",
    "\n",
    "department_count = pd.merge(prior_data, products, how=\"inner\", on=\"product_id\")\n",
    "department_count = pd.merge(department_count, aisles, how=\"inner\", on=\"aisle_id\")\n",
    "department_count = pd.merge(department_count, departments, how=\"inner\", on=\"department_id\")\n",
    "k = department_count['department'].value_counts()\n",
    "\n",
    "k = dict(k) \n",
    "department_count_dict = {i:j/len(department_count) for i , j  in k.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IKM6_712FqX"
   },
   "outputs": [],
   "source": [
    "#merging product count ratio with train data\n",
    "final_product_name = train_data['product_name'].values\n",
    "k = []\n",
    "for i in final_product_name:\n",
    "  if i in product_count_dict:\n",
    "    k.append(product_count_dict[i])\n",
    "  else:\n",
    "    k.append(0) \n",
    "final_product_name = k     \n",
    "train_data['product_name_ratio'] = final_product_name\n",
    "\n",
    "#merging aisles count ratio with train data\n",
    "final_aisle = train_data['aisle'].values\n",
    "k = []\n",
    "for i in final_aisle:\n",
    "  if i in aisles_count_dict:\n",
    "    k.append(aisles_count_dict[i])\n",
    "  else:\n",
    "    k.append(0) \n",
    "final_aisle = k \n",
    "train_data['aisle_ratio'] = final_aisle\n",
    "\n",
    "#merging department count ratio with train data\n",
    "final_department = train_data['department'].values\n",
    "k = []\n",
    "for i in final_department:\n",
    "  if i in department_count_dict:\n",
    "    k.append(department_count_dict[i])\n",
    "  else:\n",
    "    k.append(0) \n",
    "final_department = k  \n",
    "train_data['department_ratio'] = final_department\n",
    "\n",
    "#############################################################\n",
    "\n",
    "#merging product count ratio with test data\n",
    "final_product_name = test_data['product_name'].values\n",
    "k = []\n",
    "for i in final_product_name:\n",
    "  if i in product_count_dict:\n",
    "    k.append(product_count_dict[i])\n",
    "  else:\n",
    "    k.append(0) \n",
    "final_product_name = k  \n",
    "test_data['product_name_ratio'] = final_product_name\n",
    "\n",
    "#merging aisle count ratio with test data\n",
    "final_aisle = test_data['aisle'].values\n",
    "k = []\n",
    "for i in final_aisle:\n",
    "  if i in aisles_count_dict:\n",
    "    k.append(aisles_count_dict[i])\n",
    "  else:\n",
    "    k.append(0) \n",
    "final_aisle = k \n",
    "test_data['aisle_ratio'] = final_aisle\n",
    "\n",
    "#merging department count ratio with test data\n",
    "final_department = test_data['department'].values\n",
    "k = []\n",
    "for i in final_department:\n",
    "  if i in department_count_dict:\n",
    "    k.append(department_count_dict[i])\n",
    "  else:\n",
    "    k.append(0) \n",
    "final_department = k \n",
    "test_data['department_ratio'] = final_department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4_vTR3J4dH8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "in this section i am basically taking about order_dow , order_hour_of_day, days_since_prior_order , name_of_day_time\n",
    "and reorder_pattern . User purchasing product is proportion to above given attibutes. so i am counting the no of times\n",
    "user purchase item in given tie period as well as giving weightage to the feature as well .\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "all_train_order=orders[(orders['eval_set'] != 'test')]\n",
    "all_train_order=orders[(orders['eval_set'] != 'train')]\n",
    "\n",
    "k =all_train_order['order_dow'].value_counts()\n",
    "k = dict(k) \n",
    "dow_dict = {i:j/len(orders) for i , j  in k.items()}\n",
    "\n",
    "#######################################\n",
    "\n",
    "j =all_train_order['order_hour_of_day'].value_counts()\n",
    "j = dict(j)\n",
    "hour_of_day = {i:j/len(orders) for i , j  in j.items()}\n",
    "\n",
    "#########################################\n",
    "\n",
    "k = all_train_order['days_since_prior_order'].value_counts()\n",
    "k = dict(k)\n",
    "days_since_prior = {i:j/len(orders) for i , j  in k.items()}\n",
    "\n",
    "##########################################\n",
    "\n",
    "k = all_train_order['name_of_day_time'].value_counts()\n",
    "k = dict(k)\n",
    "new_name_of_day_time = {i:j/len(orders) for i , j  in k.items()}\n",
    "\n",
    "##########################################\n",
    "\n",
    "k = all_train_order['reorder_pattern'].value_counts()\n",
    "k = dict(k)\n",
    "order_reorder_pattern = {i:j/len(orders) for i , j  in k.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-8KvR2M6T7w"
   },
   "outputs": [],
   "source": [
    "#merging dow count ratio with train data\n",
    "final_dow = train_data['order_dow'].values\n",
    "final_dow = [dow_dict[i] for i in final_dow]\n",
    "train_data['dow_ratio'] = final_dow\n",
    "\n",
    "#merging hour_of_day count ratio with train data\n",
    "final_order_hour_of_day = train_data['order_hour_of_day'].values\n",
    "final_order_hour_of_day = [hour_of_day[i] for i in final_order_hour_of_day]\n",
    "train_data['hour_of_day_ratio'] = final_order_hour_of_day\n",
    "\n",
    "#merging days_since_prior count ratio with train data\n",
    "final_days_since_prior = train_data['days_since_prior_order'].values\n",
    "final_days_since_prior = [days_since_prior[i] for i in final_days_since_prior]\n",
    "train_df['days_since_prior_ratio'] = final_days_since_prior\n",
    "\n",
    "#merging reorder_pattern count ratio with train data\n",
    "final_reorder_pattern = train_df['reorder_pattern'].values\n",
    "final_reorder_pattern = [order_reorder_pattern[i] for i in final_reorder_pattern]\n",
    "train_data['reorder_pattern_ratio'] = final_reorder_pattern\n",
    "\n",
    "#merging new_name_of_day_time count ratio with train data\n",
    "final_new_name_of_day_time = train_data['name_of_day_time'].values\n",
    "final_new_name_of_day_time = [new_name_of_day_time[i] for i in final_new_name_of_day_time]\n",
    "train_data['new_name_of_day_time_ratio'] = final_new_name_of_day_time\n",
    "\n",
    "#merging dow count ratio with test data\n",
    "final_dow = test_data['order_dow'].values\n",
    "final_dow = [dow_dict[i] for i in final_dow]\n",
    "test_data['dow_ratio'] = final_dow\n",
    "\n",
    "#merging hour_of_day count ratio with test data\n",
    "final_order_hour_of_day = test_data['order_hour_of_day'].values\n",
    "final_order_hour_of_day = [hour_of_day[i] for i in final_order_hour_of_day]\n",
    "test_data['hour_of_day_ratio'] = final_order_hour_of_day\n",
    "\n",
    "#merging days_since_prior count ratio with test data\n",
    "final_days_since_prior = test_data['days_since_prior_order'].values\n",
    "final_days_since_prior = [days_since_prior[i] for i in final_days_since_prior]\n",
    "test_data['days_since_prior_ratio'] = final_days_since_prior\n",
    "\n",
    "#merging reorder_pattern count ratio with test data\n",
    "final_reorder_pattern = test_data['reorder_pattern'].values\n",
    "final_reorder_pattern = [order_reorder_pattern[i] for i in final_reorder_pattern]\n",
    "test_data['reorder_pattern_ratio'] = final_reorder_pattern\n",
    "\n",
    "#merging new_name_of_day_time count ratio with test data\n",
    "final_new_name_of_day_time = test_data['name_of_day_time'].values\n",
    "final_new_name_of_day_time = [new_name_of_day_time[i] for i in final_new_name_of_day_time]\n",
    "test_data['new_name_of_day_time_ratio'] = final_new_name_of_day_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHdgbjca9gNy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "since we are considering order_dow, order_hour_of_day, days_since_prior_order as important features . i am creating dow_by_user, \n",
    "order_hour_of_day_by_user and new_days_since_prior_order_by_user as feature adding to training data.\n",
    "\n",
    "dow_by_user := it indicates how many times particular user do shopping in given day of week.\n",
    "\n",
    "order_hour_of_day_by_user := it indicates how many times particular user do shopping in given hour of days.\n",
    "\n",
    "new_days_since_prior_order_by_user := it indicates how many times particular user do shopping in given period of month.\n",
    "\n",
    "'''\n",
    "\n",
    "all_train_order=orders[(orders['eval_set'] != 'test')]\n",
    "all_train_order=orders[(orders['eval_set'] != 'train')]\n",
    "\n",
    "order_dow = all_train_order[['user_id','order_dow']]\n",
    "new_dow = all_train_order['order_dow'].values\n",
    "order_dow['new_dow'] = new_dow\n",
    "order_dow = order_dow.groupby([\"user_id\",'order_dow'])[\"new_dow\"].aggregate(\"count\").reset_index()\n",
    "order_dow.columns = [\"user_id\", \"order_dow\", \"dow_by_user\"]\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "order_hour_of_day_by_user = all_train_order[['user_id','order_hour_of_day']]\n",
    "new_order_hour_of_day = all_train_order['order_hour_of_day'].values\n",
    "order_hour_of_day_by_user['new_order_hour_of_day'] = new_order_hour_of_day\n",
    "order_hour_of_day_by_user = order_hour_of_day_by_user.groupby([\"user_id\",'order_hour_of_day'])[\"new_order_hour_of_day\"].aggregate(\"count\").reset_index()\n",
    "order_hour_of_day_by_user.columns = [\"user_id\", \"order_hour_of_day\", \"order_hour_of_day_by_user\"]\n",
    "\n",
    "################################################################################\n",
    "\n",
    "days_since_prior_order_by_user = all_train_order[['user_id','days_since_prior_order']]\n",
    "new_days_since_prior_order = all_train_order['days_since_prior_order'].values\n",
    "days_since_prior_order_by_user['new_days_since_prior_order'] = new_days_since_prior_order\n",
    "days_since_prior_order_by_user = days_since_prior_order_by_user.groupby([\"user_id\",'days_since_prior_order'])[\"new_days_since_prior_order\"].aggregate(\"count\").reset_index()\n",
    "days_since_prior_order_by_user.columns = [\"user_id\", \"days_since_prior_order\", \"new_days_since_prior_order_by_user\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG1QppB_HsFj"
   },
   "outputs": [],
   "source": [
    "#merging order_dow with train data\n",
    "train_data = pd.merge(train_data, order_dow, how=\"left\", on=['user_id',\"order_dow\"])\n",
    "\n",
    "#merging order_hour_of_day_by_user with train data\n",
    "train_data = pd.merge(train_data, order_hour_of_day_by_user, how=\"left\", on=[\"user_id\",'order_hour_of_day'])\n",
    "\n",
    "#merging days_since_prior_order_by_user with train data\n",
    "train_data = pd.merge(train_data, days_since_prior_order_by_user, how=\"left\", on=[\"user_id\",'days_since_prior_order'])\n",
    "\n",
    "#merging order_dow with test data\n",
    "test_data = pd.merge(test_data, order_dow, how=\"left\", on=['user_id',\"order_dow\"])\n",
    "\n",
    "#merging order_hour_of_day_by_user with test data\n",
    "test_data = pd.merge(test_data, order_hour_of_day_by_user, how=\"left\", on=[\"user_id\",'order_hour_of_day'])\n",
    "\n",
    "#merging days_since_prior_order_by_user with test data\n",
    "test_data = pd.merge(test_data, days_since_prior_order_by_user, how=\"left\", on=[\"user_id\",'days_since_prior_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNR2YSdwIYh2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "same as above , i am doing further with some constrain . The constrains are user_id , product_id, order_dow, order_hour_of_day and\n",
    "days_since_prior_order . \n",
    "\n",
    "new_prouse_order_dow :=  it indicates how many times particular user purchase particular items in given day of week.\n",
    "\n",
    "new_order_hour_of_day := it indicates how many times particular user purchase particular items in given hour of day.\n",
    "\n",
    "new_days_since_prior_order := it indicates how many times particular user purchase particular items in given period of month.\n",
    "\n",
    "new_product_count_y :=  it indicates how many times particular user purchase particular items in given period of day and given day of week.\n",
    "\n",
    "new_product_count_z :=  it indicates how many times particular user purchase particular items in given period of day ,  given day of week \n",
    "and period of month.\n",
    "\n",
    "'''\n",
    "\n",
    "new_products__prior = pd.merge(prior_data, products, how=\"inner\", on=\"product_id\")\n",
    "new_products__prior = pd.merge(new_products__prior, departments, how=\"inner\", on=\"department_id\")\n",
    "new_products__prior = pd.merge(new_products__prior, aisles, how=\"inner\", on=\"aisle_id\")\n",
    "\n",
    "order_dow_by_prouse = new_products__prior[['user_id','product_id','order_dow']]\n",
    "new_prouse_order_dow = new_products__prior['order_dow'].values\n",
    "order_dow_by_prouse['new_prouse_order_dow'] = new_prouse_order_dow\n",
    "order_dow_by_prouse = order_dow_by_prouse.groupby(['user_id','product_id','order_dow'])[\"new_prouse_order_dow\"].aggregate(\"count\").reset_index()\n",
    "order_dow_by_prouse.columns = ['user_id','product_id','order_dow', 'new_prouse_order_dow']\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "order_hour_of_day_by_prouse = new_products__prior[['user_id','product_id','order_hour_of_day']]\n",
    "new_order_hour_of_day = new_products__prior['order_hour_of_day'].values\n",
    "order_hour_of_day_by_prouse['new_order_hour_of_day'] = new_order_hour_of_day\n",
    "order_hour_of_day_by_prouse = order_hour_of_day_by_prouse.groupby(['user_id','product_id','order_hour_of_day'])[\"new_order_hour_of_day\"].aggregate(\"count\").reset_index()\n",
    "order_hour_of_day_by_prouse.columns = ['user_id','product_id','order_hour_of_day', 'new_order_hour_of_day']\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "days_since_prior_order_by_prouse = new_products__prior[['user_id','product_id','days_since_prior_order']]\n",
    "new_days_since_prior_order = new_products__prior['days_since_prior_order'].values\n",
    "days_since_prior_order_by_prouse['new_days_since_prior_order'] = new_days_since_prior_order\n",
    "days_since_prior_order_by_prouse = days_since_prior_order_by_prouse.groupby(['user_id','product_id','days_since_prior_order'])[\"new_days_since_prior_order\"].aggregate(\"count\").reset_index()\n",
    "days_since_prior_order_by_prouse.columns = ['user_id','product_id','days_since_prior_order', 'new_days_since_prior_order']\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "prior_product_by_prouse_y = new_products__prior[['user_id','product_id','order_dow','name_of_day_time','product_name']]\n",
    "new_product = new_products__prior['product_name'].values\n",
    "prior_product_by_prouse_y['new_product_count'] = new_product\n",
    "prior_product_by_prouse_y = prior_product_by_prouse_y.groupby(['user_id','product_id','order_dow','name_of_day_time','product_name'])[\"new_product_count\"].aggregate(\"count\").reset_index()\n",
    "prior_product_by_prouse_y.columns = ['user_id','product_id','order_dow','name_of_day_time','product_name','new_product_count_y']\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "prior_product_by_prouse_z = new_products__prior[['user_id','product_id','days_since_prior_order','order_dow','name_of_day_time','product_name']]\n",
    "new_product = new_products__prior['product_name'].values\n",
    "prior_product_by_prouse_z['new_product_count'] = new_product\n",
    "prior_product_by_prouse_z = prior_product_by_prouse_z.groupby(['user_id','product_id','days_since_prior_order','order_dow','name_of_day_time','product_name'])[\"new_product_count\"].aggregate(\"count\").reset_index()\n",
    "prior_product_by_prouse_z.columns = ['user_id','product_id','days_since_prior_order','order_dow','name_of_day_time','product_name','new_product_count_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNIoiM1fPKX-"
   },
   "outputs": [],
   "source": [
    "#adding above given feature in train and test data\n",
    "train_data = pd.merge(train_data, order_dow_by_prouse, how=\"left\", on=['user_id','product_id','order_dow'])\n",
    "train_data = pd.merge(train_data, order_hour_of_day_by_prouse, how=\"left\", on=['user_id','product_id','order_hour_of_day'])\n",
    "train_data = pd.merge(train_data, days_since_prior_order_by_prouse, how=\"left\", on=['user_id','product_id','days_since_prior_order'])\n",
    "train_data = pd.merge(train_data, prior_product_by_prouse_y, how=\"left\", on=['user_id','product_id','order_dow','name_of_day_time','product_name'])\n",
    "train_data = pd.merge(train_data, prior_product_by_prouse_z, how=\"left\", on=['user_id','product_id','days_since_prior_order','order_dow','name_of_day_time','product_name'])\n",
    "\n",
    "test_data = pd.merge(test_data, order_dow_by_prouse, how=\"left\", on=['user_id','product_id','order_dow'])\n",
    "test_data = pd.merge(test_data, order_hour_of_day_by_prouse, how=\"left\", on=['user_id','product_id','order_hour_of_day'])\n",
    "test_data = pd.merge(test_data, days_since_prior_order_by_prouse, how=\"left\", on=['user_id','product_id','days_since_prior_order'])\n",
    "test_data = pd.merge(test_data, prior_product_by_prouse_y, how=\"left\", on=['user_id','product_id','order_dow','name_of_day_time','product_name'])\n",
    "test_data = pd.merge(test_data, prior_product_by_prouse_z, how=\"left\", on=['user_id','product_id','days_since_prior_order','order_dow','name_of_day_time','product_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7ODi7wLQUGU"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "i am creating function which in return create dataframe in which product purchased by user in give day of week is arrange in in the form of rank.\n",
    "products with lower rank indicates the product is less purchased by user whereas  higher rank indicates the product is more purchasedby user.\n",
    "in this way we can give weightage to product on the basis of day of week because user found to be doing shopping more within week.\n",
    "\n",
    "'''\n",
    "\n",
    "def rank_by_week(day):\n",
    "  orders_=orders[(orders.order_hour_of_day >= 7) & (orders.order_hour_of_day <= 20)]\n",
    "  order_by_week = orders_[['order_id','user_id','order_dow']]\n",
    "  order_prior = pd.merge(order_products_prior, order_by_week, how=\"left\", on=['order_id'])\n",
    "  order_prior = order_prior[['order_id','product_id','user_id','reordered','order_dow']]\n",
    "  order_prior = order_prior[(order_prior['reordered'] == 1)]\n",
    "\n",
    "  order_prior = order_prior[['order_id','product_id','user_id','order_dow']]\n",
    "\n",
    "  order_prior_product = pd.merge(order_prior, products, how=\"left\", on=['product_id'])\n",
    "\n",
    "  order_prior_product = order_prior_product[['user_id','order_dow','product_name']]\n",
    "  y = order_prior_product['product_name'].values\n",
    "  order_prior_product['new_product'] = y\n",
    "  order_prior_product = order_prior_product[(order_prior_product['order_dow'] == day)]\n",
    "\n",
    "  # get the count of each product and number of reorders by the customer #\n",
    "  order_prior_product_groupby = order_prior_product.groupby([\"user_id\",\"order_dow\",'product_name'])[\"new_product\"].aggregate([\"count\"]).reset_index()\n",
    "  order_prior_product_groupby.columns = [\"user_id\", \"order_dow\", \"product_name\", \"product_count\"]\n",
    "\n",
    "  j = np.array([i for i in order_prior_product_groupby['user_id'].value_counts()])\n",
    "\n",
    "  p = np.percentile(j, 50)\n",
    "\n",
    "  order_prior_product_groupby['rank'+str(day)] = order_prior_product_groupby.groupby([\"user_id\",\"order_dow\"])['product_count'].rank(ascending=True,method='dense')\n",
    "\n",
    "  order_prior_product_groupby = order_prior_product_groupby[(order_prior_product_groupby['rank'+str(day)] <= p)]\n",
    "  order_prior_product_groupby = order_prior_product_groupby[[\"user_id\",\"order_dow\",'product_name','rank'+str(day)]]\n",
    "  return order_prior_product_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T17iAtvWSdui"
   },
   "outputs": [],
   "source": [
    "order_prior_product_day0 = rank_by_week(0)\n",
    "order_prior_product_day1 = rank_by_week(1)\n",
    "order_prior_product_day2 = rank_by_week(2)\n",
    "order_prior_product_day3 = rank_by_week(3)\n",
    "order_prior_product_day4 = rank_by_week(4)\n",
    "order_prior_product_day5 = rank_by_week(5)\n",
    "order_prior_product_day6 = rank_by_week(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pALdxfNXShqG"
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, order_prior_product_day0, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "train_data = pd.merge(train_data, order_prior_product_day1, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "train_data = pd.merge(train_data, order_prior_product_day2, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "train_data = pd.merge(train_data, order_prior_product_day3, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "train_data = pd.merge(train_data, order_prior_product_day4, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "train_data = pd.merge(train_data, order_prior_product_day5, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "train_data = pd.merge(train_data, order_prior_product_day6, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "\n",
    "test_data = pd.merge(test_data, order_prior_product_day0, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "test_data = pd.merge(test_data, order_prior_product_day1, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "test_data = pd.merge(test_data, order_prior_product_day2, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "test_data = pd.merge(test_data, order_prior_product_day3, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "test_data = pd.merge(test_data, order_prior_product_day4, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "test_data = pd.merge(test_data, order_prior_product_day5, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])\n",
    "test_data = pd.merge(test_data, order_prior_product_day6, how=\"left\", on=[\"user_id\",\"order_dow\",'product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqkUb-TwS0Fm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "i am capturing the time in sec from 1 day of month the particular product is purchased .\n",
    "\n",
    "'''\n",
    "\n",
    "train_data['time_in_sec'] = train_data['days_since_prior_order']*24*60*60 + train_df['order_hour_of_day']*60*60\n",
    "test_data['time_in_sec'] = test_data['days_since_prior_order']*24*60*60 + test_df['order_hour_of_day']*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLnYkDq2TfuG"
   },
   "outputs": [],
   "source": [
    "#creating one hot encoding of reorder_pattern and name_of_day_time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "t_train = train_data[['reorder_pattern', 'name_of_day_time']]\n",
    "t_test = test_data[['reorder_pattern', 'name_of_day_time']]\n",
    "\n",
    "pattern = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "pattern.fit(t_train[['reorder_pattern']])\n",
    "a_train = pattern.transform(t_train[['reorder_pattern']])\n",
    "a_test = pattern.transform(t_test[['reorder_pattern']])\n",
    "\n",
    "pattern1 = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "pattern1.fit(t_train[['name_of_day_time']])\n",
    "b_train = pattern1.transform(t_train[['name_of_day_time']])\n",
    "b_test = pattern1.transform(t_test[['name_of_day_time']])\n",
    "\n",
    "a_train = a_train.toarray()\n",
    "a_test = a_test.toarray()\n",
    "b_train = b_train.toarray()\n",
    "b_test = b_test.toarray()\n",
    "\n",
    "m_train = np.concatenate((a_train, b_train), axis=1)\n",
    "hot_train = pd.DataFrame(data = m_train,  index = None,  columns = ['1','2','3','4','5','6','7','8'])\n",
    "\n",
    "m_test = np.concatenate((a_test, b_test), axis=1)\n",
    "hot_test = pd.DataFrame(data = m_test,  index = None,  columns = ['1','2','3','4','5','6','7','8'])\n",
    "\n",
    "\n",
    "train_data = pd.concat([train_data, hot_train], axis=1)\n",
    "test_data = pd.concat([test_data, hot_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsX0L3e5WEtm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "in training as well as test dataset , products has been associates with user on which we have to tari and test the model.\n",
    "i am capturing the feature which tell us that if the particular product had been bought by the user previously same time frame. \n",
    "\n",
    "Ex := if in train dataset if the product P is associate with user A and given time frame is C dow , D hour , E days_since_prior_order then , i want\n",
    "to know if the same product P had user A purchased in same C or D or E time frame in previous purchase history and if answer is yes , \n",
    "then 'exit' indicates 1 or if No then 0. \n",
    "'''\n",
    "\n",
    "ord_prior = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "ord_prior_pro = pd.merge(ord_prior, products, how=\"inner\", on=['product_id'])\n",
    "use_hour_pro = ord_prior_pro[['user_id','order_hour_of_day','product_name']]\n",
    "use_hour_pro['exit'] = [1]*use_hour_pro.shape[0]\n",
    "use_hour_pro.to_csv('use_hour_pro.csv')\n",
    "\n",
    "################################################################################\n",
    "\n",
    "ord_prior = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "ord_prior_pro = pd.merge(ord_prior, products, how=\"inner\", on=['product_id'])\n",
    "use_pattern_pro = ord_prior_pro[['user_id','name_of_day_time','product_name']]\n",
    "use_pattern_pro['exit'] = [1]*use_pattern_pro.shape[0]\n",
    "use_pattern_pro.to_csv('use_pattern_pro.csv')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "ord_prior = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "ord_prior_pro = pd.merge(ord_prior, products, how=\"inner\", on=['product_id'])\n",
    "use_dow_pro = ord_prior_pro[['user_id','order_dow','product_name']]\n",
    "use_dow_pro['exit'] = [1]*use_dow_pro.shape[0]\n",
    "use_dow_pro.to_csv('use_dow_pro.csv')\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "ord_prior = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "ord_prior_pro = pd.merge(ord_prior, products, how=\"inner\", on=['product_id'])\n",
    "use_days_since_pro = ord_prior_pro[['user_id','days_since_prior_order','product_name']]\n",
    "use_days_since_pro['exit'] = [1]*use_days_since_pro.shape[0]\n",
    "use_days_since_pro.to_csv('use_days_since_pro.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8HoCNSuZKrg"
   },
   "outputs": [],
   "source": [
    "use_hour_pro.to_csv('use_hour_pro.csv')\n",
    "use_pattern_pro.to_csv('use_pattern_pro.csv')\n",
    "use_dow_pro.to_csv('use_dow_pro.csv')\n",
    "use_days_since_pro.to_csv('use_days_since_pro.csv')\n",
    "use_pro_count.to_csv('use_pro_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnrqOsY7ZlYN"
   },
   "outputs": [],
   "source": [
    "name2 = ['user_id', 'days_since_prior_order', 'product_name', 'exit']\n",
    "name3 = ['user_id', 'order_hour_of_day', 'product_name', 'exit']\n",
    "name4 = ['user_id', 'name_of_day_time', 'product_name', 'exit']\n",
    "name5 = ['user_id', 'order_dow', 'product_name', 'exit']\n",
    "\n",
    "use_days_since_pro = pd.read_csv('use_days_since_pro.csv', usecols= name2)\n",
    "use_days_since_pro = use_days_since_pro.drop_duplicates()\n",
    "use_days_since_pro.columns = ['user_id', 'days_since_prior_order', 'product_name', 'exit1']\n",
    "\n",
    "use_hour_pro = pd.read_csv('use_hour_pro.csv', usecols= name3)\n",
    "use_hour_pro = use_hour_pro.drop_duplicates()\n",
    "use_hour_pro.columns = ['user_id', 'order_hour_of_day', 'product_name', 'exit2']\n",
    "\n",
    "use_pattern_pro = pd.read_csv('use_pattern_pro.csv', usecols= name4)\n",
    "use_pattern_pro = use_pattern_pro.drop_duplicates()\n",
    "use_pattern_pro.columns = ['user_id', 'name_of_day_time', 'product_name', 'exit3']\n",
    "\n",
    "use_dow_pro = pd.read_csv('use_dow_pro.csv', usecols= name5)\n",
    "use_dow_pro = use_dow_pro.drop_duplicates()\n",
    "use_dow_pro.columns = ['user_id', 'order_dow', 'product_name', 'exit4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pthkgyC1aNS8"
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, use_days_since_pro, how=\"left\", on=['user_id', 'days_since_prior_order', 'product_name'])\n",
    "train_data = pd.merge(train_data, use_hour_pro, how=\"left\", on=['user_id', 'order_hour_of_day', 'product_name'])\n",
    "train_data = pd.merge(train_data, use_pattern_pro, how=\"left\", on=['user_id', 'name_of_day_time', 'product_name'])\n",
    "train_data = pd.merge(train_data, use_dow_pro, how=\"left\", on=['user_id', 'order_dow', 'product_name'])\n",
    "\n",
    "test_data = pd.merge(test_data, use_days_since_pro, how=\"left\", on=['user_id', 'days_since_prior_order', 'product_name'])\n",
    "test_data = pd.merge(test_data, use_hour_pro, how=\"left\", on=['user_id', 'order_hour_of_day', 'product_name'])\n",
    "test_data = pd.merge(test_data, use_pattern_pro, how=\"left\", on=['user_id', 'name_of_day_time', 'product_name'])\n",
    "test_data = pd.merge(test_data, use_dow_pro, how=\"left\", on=['user_id', 'order_dow', 'product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u69TGUHAbyQa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "since there are 3 attributes in dataframe which include product name, aisles name , department name .  i am joining 3 column together group by user id \n",
    "and then perfforming average W2V on top of that just to see if it could boost the model because user number of times purchasing item influence the\n",
    "reorder pattern.  \n",
    "'''\n",
    "def word_concat():\n",
    "  z = []\n",
    "  name = ['product_name','department','aisle']\n",
    "  for i in range(3):\n",
    "    if i == 0:\n",
    "      pro_order = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "      pro_order = pd.merge(pro_order, products, how=\"inner\", on=['product_id'])\n",
    "      user_product = pro_order[['user_id','product_name']]\n",
    "\n",
    "    if i == 1:\n",
    "      pro_order = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "      pro_order = pd.merge(pro_order, products, how=\"inner\", on=['product_id'])\n",
    "      pro_order = pd.merge(pro_order, departments, how=\"inner\", on=['department_id'])\n",
    "      user_product = pro_order[['user_id','department']]\n",
    "\n",
    "    if i == 2:\n",
    "      pro_order = pd.merge(order_products_prior, orders, how=\"inner\", on=['order_id'])\n",
    "      pro_order = pd.merge(pro_order, products, how=\"inner\", on=['product_id'])\n",
    "      pro_order = pd.merge(pro_order, aisles, how=\"inner\", on=['aisle_id'])\n",
    "      user_product = pro_order[['user_id','aisle']]\n",
    "\n",
    "    user_unique = list(set(user_product['user_id'].values))  \n",
    "\n",
    "    product_id_name = []\n",
    "    for j in user_unique:\n",
    "      li = []\n",
    "      k = user_product[(user_product['user_id'] == j)]\n",
    "      li = list(k[name[i]].values)\n",
    "      product_id_name.append(li) \n",
    "\n",
    "    product_id_name = [' '.join(\"\".join(e.split()) for e in k)  for k in product_id_name] \n",
    "    z.append(product_id_name)\n",
    "\n",
    "\n",
    "  data = {'user_id':user_unique, 'sentence0':z[0],'sentence1':z[1],'sentence2':z[2]} \n",
    "  sen = pd.DataFrame(data)\n",
    "  sen['all_sentence'] = sen['sentence0'] +' '+ sen['sentence1'] +' '+ sen['sentence2']\n",
    "  sen = sen[['user_id','all_sentence']]\n",
    "\n",
    "  return sen\n",
    "\n",
    "sen = word_concat()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O3F6ySU1msK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "i am doing Average W2V instead of bag of wof or Tfidf because w2v can help us predict the similar product,  aisles and department name.\n",
    "\n",
    "'''\n",
    "\n",
    "# Training our own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_sentance=[]\n",
    "for sentance in sen['all_sentence'].values:\n",
    "    list_sentance.append(sentance.split())\n",
    "\n",
    "model = gensim.models.Word2Vec(list_of_sentance, size=50, min_count=1, workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_train = train_data.replace(' ','',regex=True)\n",
    "new_data_train['all_sen'] = new_data_train['department'] +' '+ new_data_train['aisle'] + ' ' +new_data_train['product_name']\n",
    "\n",
    "new_data_test = test_data.replace(' ','',regex=True)\n",
    "new_data_test['all_sen'] = new_data_test['department'] +' '+ new_data_test['aisle'] + ' ' +new_data_test['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "i=0\n",
    "list_sentance=[]\n",
    "for sentance in new_data_train['all_sen'].values:\n",
    "    list_sentance.append(sentance.split())\n",
    "\n",
    "\n",
    "sentence_vector_train = []\n",
    "for sentence in list_sentance:\n",
    "  sent_vectors = np.zeros(5)\n",
    "  count = 0\n",
    "  for word in sentence:\n",
    "    if word in model:\n",
    "      vector = model.wv[word]\n",
    "      sent_vectors += vector\n",
    "      count += 1\n",
    "  if count != 0:\n",
    "    sent_vectors /= count\n",
    "  sentence_vector_train.append(sent_vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "i=0\n",
    "list_sentance=[]\n",
    "for sentance in new_data_test['all_sen'].values:\n",
    "    list_sentance.append(sentance.split())\n",
    "\n",
    "sentence_vector_test = []\n",
    "for sentence in list_sentance:\n",
    "  sent_vectors = np.zeros(5)\n",
    "  count = 0\n",
    "  for word in sentence:\n",
    "    if word in model:\n",
    "      vector = model.wv[word]\n",
    "      sent_vectors += vector\n",
    "      count += 1\n",
    "  if count != 0:\n",
    "    sent_vectors /= count\n",
    "  sentence_vector_test.append(sent_vectors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_name = ['B'+str(i) for i in range(5)]\n",
    "w2v_train = pd.DataFrame(data = sentence_vector_train,  index = None,  columns = w2v_name)\n",
    "\n",
    "w2v_test = pd.DataFrame(data = sentence_vector_test,  index = None,  columns = w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QD8b9T2C3Xcw"
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data,w2v_train], axis=1)\n",
    "test_data = pd.concat([test_data,w2v_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in this section , i am creating feature regarding how much percentage of particular product is reorder by user in particular \n",
    "day of week or hour of day or day of month. it is carried out for Active customer which mean from the graph , on every \n",
    "weekend i.e 7 , 14, 21 and 28 , there are some user who wish to buy their products for whole week.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "li = [['user_id','order_dow','product_name'],['user_id','order_hour_of_day','product_name'],['user_id','days_since_prior_order','product_name'],\n",
    "      ['user_id','reorder_pattern','product_name'],['user_id','name_of_day_time','product_name'],['user_id','order_dow','order_hour_of_day','product_name'],\n",
    "     ['user_id','order_dow','days_since_prior_order','product_name'],['user_id','reorder_pattern','order_hour_of_day','product_name'],\n",
    "      ['user_id','name_of_day_time','days_since_prior_order','product_name'],['user_id','order_dow','order_hour_of_day','days_since_prior_order','product_name',]]\n",
    "\n",
    "merged_data = prior_data[(prior_data['days_since_prior_order'] == 7) | (prior_data['days_since_prior_order'] == 14) | \n",
    "                (prior_data['days_since_prior_order'] == 21) | (prior_data['days_since_prior_order'] == 28) ]\n",
    "\n",
    "for i in range(10):\n",
    "  a = merged_data.groupby(li[i])[\"new_product\"].aggregate(\"count\").reset_index()\n",
    "  b = a.groupby(li[i][:-1])[\"new_product\"].apply(lambda x:  x / x.sum())\n",
    "  a['per'+str(i)] = b.to_list()\n",
    "  c = a[li[i]+['per'+str(i)]]\n",
    "  new_train = pd.merge(new_train,c,on=li[i],how = 'left')\n",
    "  new_test = pd.merge(new_test,c,on=li[i],how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing one hot encoding for 'reorder_pattern' and 'name_of_day_time'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "t_train = train_data[['reorder_pattern', 'name_of_day_time']]\n",
    "t_test = test_data[['reorder_pattern', 'name_of_day_time']]\n",
    "\n",
    "pattern = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "pattern.fit(t_train[['reorder_pattern']])\n",
    "a_train = pattern.transform(t_train[['reorder_pattern']])\n",
    "a_test = pattern.transform(t_test[['reorder_pattern']])\n",
    "\n",
    "pattern1 = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "pattern1.fit(t_train[['name_of_day_time']])\n",
    "b_train = pattern1.transform(t_train[['name_of_day_time']])\n",
    "b_test = pattern1.transform(t_test[['name_of_day_time']])\n",
    "\n",
    "a_train = a_train.toarray()\n",
    "a_test = a_test.toarray()\n",
    "b_train = b_train.toarray()\n",
    "b_test = b_test.toarray()\n",
    "\n",
    "m_train = np.concatenate((a_train, b_train), axis=1)\n",
    "hot_train = pd.DataFrame(data = m_train,  index = None,  columns = ['1','2','3','4','5','6','7','8'])\n",
    "\n",
    "m_test = np.concatenate((a_test, b_test), axis=1)\n",
    "hot_test = pd.DataFrame(data = m_test,  index = None,  columns = ['1','2','3','4','5','6','7','8'])\n",
    "\n",
    "\n",
    "train_data = pd.concat([train_data, hot_train], axis=1)\n",
    "test_data = pd.concat([test_data, hot_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlNMNnCI-D0y"
   },
   "outputs": [],
   "source": [
    "#creating feature which tell us how many time particular product has been reorder in last 5 order by user\n",
    "train_reorder_status_5 = train_data['reordered_latest1'].fillna(0) + train_data['reordered_latest2'].fillna(0)+ train_data['reordered_latest3'].fillna(0) + \\\n",
    "                   train_data['reordered_latest4'].fillna(0) + train_data['reordered_latest5'].fillna(0)\n",
    "\n",
    "train_data['reorder_status_5'] = train_reorder_status_5   \n",
    "\n",
    "test_reorder_status_5 = test_data['reordered_latest1'].fillna(0) + test_data['reordered_latest2'].fillna(0)+ test_data['reordered_latest3'].fillna(0) + \\\n",
    "                   test_data['reordered_latest4'].fillna(0) + test_data['reordered_latest5'].fillna(0)\n",
    "\n",
    "test_data['reorder_status_5'] = test_reorder_status_5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"order_products__train.csv\", usecols=[\"order_id\", \"product_id\", \"reordered\"])\n",
    "train = pd.merge(train, orders, how=\"inner\", on=\"order_id\")\n",
    "train = train[[\"user_id\", \"product_id\", \"reordered\"]]\n",
    "train_data = pd.merge(train_data, train, how=\"left\", on=[\"user_id\", \"product_id\"])\n",
    "train_data[\"reordered\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =train_df['reordered']\n",
    "Y.to_csv('Y.csv')\n",
    "train_data.to_csv('train_data.csv')\n",
    "test_data.to_csv('test_data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "instacart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
